{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spam Classification \n",
    "In this part of the exercise, you will use SVMs to build your own spam filter. You will be training a classifier to classify whether a given email, x, is spam (y = 1) or non-spam (y = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing E-mails\n",
    "To use an SVM to classify emails into Spam v.s. Non-Spam, you first need to convert each email into a vector of features. In this part, you should produce a word indices vector for a given email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import svm\n",
    "from pandas import DataFrame as dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Part 1: Email Preprocessing ====================\n",
    "# Several functions need to be built to support processEmail, such as vocabList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabList():\n",
    "    # GETVOCABLIST reads the fixed vocabulary list in vocab.txt and returns a\n",
    "    # cell array of the words\n",
    "    #     vocabList = GETVOCABLIST() reads the fixed vocabulary list in vocab.txt \n",
    "    #     and returns a cell array of the words in vocabList.\n",
    "    \n",
    "    vocabList = {}\n",
    "    \n",
    "    with open('vocab.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            key, value = line.split()\n",
    "            vocabList[int(key)-1] = value\n",
    "            \n",
    "    return vocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Vocabulary List\n",
    "After preprocessing the emails, we have a list of words for each email. \n",
    "\n",
    "We have chosen only the most frequently occuring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to overfit our training set. The complete vocabulary list is in the file vocab.txt \n",
    "\n",
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list. For example, in the sample email, the word \\anyone\" was first normalized to \"anyone\" and then mapped onto the index 86 in the vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabList = getVocabList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmail(email_contents):\n",
    "    '''\n",
    "    PROCESSEMAIL preprocesses a the body of an email and\n",
    "    returns a list of word_indices \n",
    "       word_indices = PROCESSEMAIL(email_contents) preprocesses \n",
    "       the body of an email and returns a list of indices of the \n",
    "       words contained in the email. \n",
    "    '''\n",
    "\n",
    "    \n",
    "    #  ========================== Preprocess Email ===========================\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # and does not have any < or > in the tag it with a space\n",
    "    email_contents = re.sub('^<[^<>]+>$',' ', email_contents) \n",
    "    \n",
    "    # Handle Numbers\n",
    "    # Look for one or more characters between 0-9\n",
    "    email_contents = re.sub('[0-9]+', 'number', email_contents)\n",
    "    \n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https://\n",
    "    email_contents = re.sub('(http|https)://[^\\s]*', 'httpaddr', email_contents)\n",
    "    \n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email_contents)\n",
    "    \n",
    "    # Handle $ sign\n",
    "    email_contents = re.sub('[$]+', 'dollar', email_contents)\n",
    "    \n",
    "    # ========================== Tokenize Email ===========================\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokenized = word_tokenize(email_contents)\n",
    "    \n",
    "    # Return indices of words that appear in the content\n",
    "    word_indices = list()\n",
    "    \n",
    "    # Define porter stemmer\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # Stem the words and match it to dictionary vocabList\n",
    "    for i in range(len(tokenized)):\n",
    "        stemmed = ps.stem(tokenized[i])\n",
    "    \n",
    "        # Remove any non alphanumeric characters\n",
    "        stemmed = re.sub('[^a-zA-Z0-9]', '', stemmed)\n",
    "\n",
    "        # Look up the word in the dictionary and add to word_indices if exists\n",
    "        m = len(vocabList)\n",
    "    \n",
    "        for i in range(m):\n",
    "            if stemmed == vocabList[i]:\n",
    "                word_indices.append(i)\n",
    "            \n",
    "    \n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read email sample\n",
    "with open('emailSample1.txt') as file:\n",
    "    email_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "word_indices = processEmail(email_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indices:\n",
      "\n",
      "[85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 882, 430, 1170, 793, 1001, 1892, 1363, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 1698, 374, 1161, 478, 1892, 1509, 798, 1181, 1236, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 687, 1675, 991, 960, 1476, 70, 529, 1698, 530]\n"
     ]
    }
   ],
   "source": [
    "# Print stats\n",
    "print('Word indices:\\n')\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extracting Features from Emails\n",
    "You will now implement the feature extraction that converts each email into\n",
    "a vector in $\\mathbb{R}^n$. For this exercise, you will be using *n = # words* in vocabularylist.\n",
    "\n",
    "Specifically, the feature $x_{i} {\\in} \\{0, 1\\}$ for an email corresponds to whether the *i-th* \n",
    "word in the dictionary occurs in the email\n",
    "![spam_extracted_feat](./img/spam_extracted_feat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Part 2: Feature Extraction ====================\n",
    "# This part will convert each email into a vector of feature R^n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "    # Number of words in the dictionary\n",
    "    n = len(vocabList)\n",
    "    \n",
    "    # Return the feature vector x correctly\n",
    "    x = np.zeros([n, 1])\n",
    "    \n",
    "    # New feature vector, 1 is substracted to conform with python indexing\n",
    "    x[word_indices] = 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "file_contents = email_contents\n",
    "word_indices = processEmail(file_contents)\n",
    "features = emailFeatures(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1899 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "...   ...\n",
       "1894  1.0\n",
       "1895  1.0\n",
       "1896  0.0\n",
       "1897  0.0\n",
       "1898  0.0\n",
       "\n",
       "[1899 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: 45\n"
     ]
    }
   ],
   "source": [
    "# Print stats\n",
    "print('Length of feature vector: %d' %len(features))\n",
    "print('Number of non-zero entries: %d' %sum(features>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Part 3: Train Linear SVM for Spam Classification ========\n",
    "# This section will train a linear classifier to determine if an email is Spam or Not-Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spam email train dataset\n",
    "spamTrain = sio.loadmat('spamTrain.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spamTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamTrain.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spamTrain['X']\n",
    "y = spamTrain['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 1899), (4000, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up classifier\n",
    "clf = svm.SVC(C=0.1, kernel ='linear')\n",
    "model = clf.fit(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on train data\n",
    "p = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.825\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: %.3f' %(np.mean(p==y.flatten())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Part 4: Test Spam Classification ================\n",
    "# This section will evaluate trained model on a test set. Test set is included in spamTest.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spam email train dataset\n",
    "spamTest = sio.loadmat('spamTrain.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spamTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamTest.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = spamTest['X']\n",
    "ytest = spamTest['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "p = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.825\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: %.3f' %(np.mean(p==ytest.flatten())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Top Predictors for Spam\n",
    "To better understand how the spam classi\f",
    "er works, we can inspect the parameters to see which words the classifier thinks are the most predictive of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Part 5: Top Predictors of Spam ====================\n",
    "# This section will inspect the learned weights to get better understanding how this model determine whether or not an email \n",
    "# is spam. The highest value of weight will be printed, this weights is the most likely indicator of spam email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of weights\n",
    "weights = model.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the weights and obtain vocabulary list\n",
    "idx = np.argsort(weights)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1190,  297, 1397, ..., 1764, 1665, 1560], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictors of span:\n",
      "\n",
      "our 0.5006137361746409\n",
      "click 0.4659163906888878\n",
      "remov 0.4228691170610411\n",
      "guarante 0.38362160179406524\n",
      "visit 0.367710398245535\n",
      "basenumb 0.3450640979461706\n",
      "dollar 0.3236320357963838\n",
      "will 0.2697241060374009\n",
      "price 0.26729771461770707\n",
      "pleas 0.261168886700149\n",
      "most 0.25729819795181635\n",
      "nbsp 0.2539414551595325\n",
      "lo 0.25346652431419925\n",
      "ga 0.24829699045568662\n",
      "hour 0.24640435783158998\n",
      "al 0.23731066817215565\n"
     ]
    }
   ],
   "source": [
    "# Note that due to calculation precision the result might differ from MATLAB result\n",
    "print('Top predictors of span:\\n')\n",
    "for i in idx[:16]:\n",
    "    print(vocabList[i], weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Try your own emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### =================== Part 6: Try Your Own Emails =====================\n",
    "# The trained classifier to test your own email, paste your own email to spamSample2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change this to any file name to see how it is predicted\n",
    "filename = 'spamSample2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename) as file: \n",
    "    file_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and predict\n",
    "word_indices = processEmail(file_contents)\n",
    "x = emailFeatures(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1899)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since x only contains one sample of 1899 feature\n",
    "# x needs to be transposed so that it is in shape of 1 x 1899\n",
    "x = x.T\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict x\n",
    "p = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: spamSample2.txt\n",
      "Spam Classification: 1\n",
      "\n",
      "1 indicates spam, 0 indicates not spam)\n"
     ]
    }
   ],
   "source": [
    "print('Processed: %s' %filename)\n",
    "print('Spam Classification: %d\\n' %p)\n",
    "print('1 indicates spam, 0 indicates not spam)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
